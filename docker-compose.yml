services:
  # MySQL Database
  db:
    image: mysql:8.0
    container_name: search_analytics_db
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: search_analytics
      MYSQL_USER: user
      MYSQL_PASSWORD: password
    ports:
      - "3306:3306"
    volumes:
      - ./data/mysql_data:/var/lib/mysql
      - ./mysql-init:/docker-entrypoint-initdb.d
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-ppassword"]
      interval: 5s
      timeout: 5s
      retries: 10

  # FastAPI Service
  api:
    build:
      context: .
      dockerfile: ./app/Dockerfile
    container_name: search_analytics_api
    restart: always
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "5000:5000"
    environment:
      MYSQL_HOST: db
      MYSQL_USER: user
      MYSQL_PASSWORD: password
      MYSQL_DATABASE: search_analytics
    volumes:
      - ./app:/app

  # Data Generator (runs once to populate database)
  data_generator:
    build:
      context: .
      dockerfile: ./app/Dockerfile
    container_name: search_analytics_data_generator
    restart: "no"
    depends_on:
      db:
        condition: service_healthy
    environment:
      MYSQL_HOST: db
      MYSQL_USER: user
      MYSQL_PASSWORD: password
      MYSQL_DATABASE: search_analytics
    volumes:
      - ./app:/app
    command: ["python", "data_generator.py"]

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      db:
        condition: service_healthy
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqlconnector://root:password@db:3306/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    command: version
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin"

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - airflow-init
      - db
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqlconnector://root:password@db:3306/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - airflow-init
      - db
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=mysql+mysqlconnector://root:password@db:3306/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark:7077
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  spark:
    image: bitnami/spark:3.3.2
    environment:
      - SPARK_MODE=master
    ports:
      - "8181:8080"
    volumes:
      - ./dags/spark:/opt/spark/work

  spark-worker:
    image: bitnami/spark:3.3.2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark
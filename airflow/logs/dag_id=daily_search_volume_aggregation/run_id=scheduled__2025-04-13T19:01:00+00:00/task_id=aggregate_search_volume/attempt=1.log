[2025-04-13T20:01:01.329+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:01:01.341+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:01:01.342+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-04-13T20:01:01.354+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): aggregate_search_volume> on 2025-04-13 19:01:00+00:00
[2025-04-13T20:01:01.358+0000] {standard_task_runner.py:57} INFO - Started process 223 to run task
[2025-04-13T20:01:01.362+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'daily_search_volume_aggregation', 'aggregate_search_volume', 'scheduled__2025-04-13T19:01:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/daily_search_volume_aggregation.py', '--cfg-path', '/tmp/tmpbtp0hst4']
[2025-04-13T20:01:01.365+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask aggregate_search_volume
[2025-04-13T20:01:01.427+0000] {task_command.py:415} INFO - Running <TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [running]> on host 0598501f5f50
[2025-04-13T20:01:01.510+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='daily_search_volume_aggregation' AIRFLOW_CTX_TASK_ID='aggregate_search_volume' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T19:01:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T19:01:00+00:00'
[2025-04-13T20:01:01.511+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2025-04-13T20:01:01.512+0000] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master spark:7077 --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.jars.packages=mysql:mysql-connector-java:8.0.28 --name arrow-spark --verbose /opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:01:04.962+0000] {spark_submit.py:491} INFO - Using properties file: null
[2025-04-13T20:01:05.119+0000] {spark_submit.py:491} INFO - Parsed arguments:
[2025-04-13T20:01:05.119+0000] {spark_submit.py:491} INFO - master                  spark:7077
[2025-04-13T20:01:05.119+0000] {spark_submit.py:491} INFO - remote                  null
[2025-04-13T20:01:05.119+0000] {spark_submit.py:491} INFO - deployMode              null
[2025-04-13T20:01:05.120+0000] {spark_submit.py:491} INFO - executorMemory          1g
[2025-04-13T20:01:05.120+0000] {spark_submit.py:491} INFO - executorCores           null
[2025-04-13T20:01:05.120+0000] {spark_submit.py:491} INFO - totalExecutorCores      null
[2025-04-13T20:01:05.120+0000] {spark_submit.py:491} INFO - propertiesFile          null
[2025-04-13T20:01:05.120+0000] {spark_submit.py:491} INFO - driverMemory            1g
[2025-04-13T20:01:05.121+0000] {spark_submit.py:491} INFO - driverCores             null
[2025-04-13T20:01:05.121+0000] {spark_submit.py:491} INFO - driverExtraClassPath    null
[2025-04-13T20:01:05.121+0000] {spark_submit.py:491} INFO - driverExtraLibraryPath  null
[2025-04-13T20:01:05.121+0000] {spark_submit.py:491} INFO - driverExtraJavaOptions  null
[2025-04-13T20:01:05.121+0000] {spark_submit.py:491} INFO - supervise               false
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - queue                   null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - numExecutors            null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - files                   null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - pyFiles                 null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - archives                null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - mainClass               null
[2025-04-13T20:01:05.122+0000] {spark_submit.py:491} INFO - primaryResource         file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - name                    arrow-spark
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - childArgs               []
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - jars                    null
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - packages                mysql:mysql-connector-java:8.0.28
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - packagesExclusions      null
[2025-04-13T20:01:05.123+0000] {spark_submit.py:491} INFO - repositories            null
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - verbose                 true
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - Spark properties used, including those specified through
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - --conf and those from the properties file null:
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:01:05.124+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:01:05.125+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:01:05.125+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:01:05.125+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:01:05.270+0000] {spark_submit.py:491} INFO - :: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-04-13T20:01:05.525+0000] {spark_submit.py:491} INFO - Ivy Default Cache set to: /home/airflow/.ivy2/cache
[2025-04-13T20:01:05.526+0000] {spark_submit.py:491} INFO - The jars for the packages stored in: /home/airflow/.ivy2/jars
[2025-04-13T20:01:05.534+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java added as a dependency
[2025-04-13T20:01:05.537+0000] {spark_submit.py:491} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-45863119-ee95-49eb-a1c3-b0e5a93e4fce;1.0
[2025-04-13T20:01:05.538+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:01:07.283+0000] {spark_submit.py:491} INFO - found mysql#mysql-connector-java;8.0.28 in central
[2025-04-13T20:01:10.559+0000] {spark_submit.py:491} INFO - found com.google.protobuf#protobuf-java;3.11.4 in central
[2025-04-13T20:01:10.784+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar ...
[2025-04-13T20:01:13.083+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] mysql#mysql-connector-java;8.0.28!mysql-connector-java.jar (2490ms)
[2025-04-13T20:01:13.283+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.11.4/protobuf-java-3.11.4.jar ...
[2025-04-13T20:01:14.521+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] com.google.protobuf#protobuf-java;3.11.4!protobuf-java.jar(bundle) (1429ms)
[2025-04-13T20:01:14.529+0000] {spark_submit.py:491} INFO - :: resolution report :: resolve 5046ms :: artifacts dl 3945ms
[2025-04-13T20:01:14.530+0000] {spark_submit.py:491} INFO - :: modules in use:
[2025-04-13T20:01:14.531+0000] {spark_submit.py:491} INFO - com.google.protobuf#protobuf-java;3.11.4 from central in [default]
[2025-04-13T20:01:14.532+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java;8.0.28 from central in [default]
[2025-04-13T20:01:14.533+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:01:14.534+0000] {spark_submit.py:491} INFO - |                  |            modules            ||   artifacts   |
[2025-04-13T20:01:14.535+0000] {spark_submit.py:491} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-04-13T20:01:14.535+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:01:14.536+0000] {spark_submit.py:491} INFO - |      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
[2025-04-13T20:01:14.536+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:01:14.549+0000] {spark_submit.py:491} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-45863119-ee95-49eb-a1c3-b0e5a93e4fce
[2025-04-13T20:01:14.550+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:01:14.581+0000] {spark_submit.py:491} INFO - 2 artifacts copied, 0 already retrieved (4040kB/32ms)
[2025-04-13T20:01:14.923+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-04-13T20:01:15.289+0000] {spark_submit.py:491} INFO - Main class:
[2025-04-13T20:01:15.290+0000] {spark_submit.py:491} INFO - org.apache.spark.deploy.PythonRunner
[2025-04-13T20:01:15.290+0000] {spark_submit.py:491} INFO - Arguments:
[2025-04-13T20:01:15.290+0000] {spark_submit.py:491} INFO - file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:01:15.290+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:01:15.296+0000] {spark_submit.py:491} INFO - Spark config:
[2025-04-13T20:01:15.296+0000] {spark_submit.py:491} INFO - (spark.app.name,arrow-spark)
[2025-04-13T20:01:15.296+0000] {spark_submit.py:491} INFO - (spark.app.submitTime,1744574475250)
[2025-04-13T20:01:15.296+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:01:15.296+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:01:15.297+0000] {spark_submit.py:491} INFO - (spark.files,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:01:15.297+0000] {spark_submit.py:491} INFO - (spark.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:01:15.297+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:01:15.297+0000] {spark_submit.py:491} INFO - (spark.master,spark:7077)
[2025-04-13T20:01:15.297+0000] {spark_submit.py:491} INFO - (spark.repl.local.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:01:15.298+0000] {spark_submit.py:491} INFO - (spark.submit.deployMode,client)
[2025-04-13T20:01:15.298+0000] {spark_submit.py:491} INFO - (spark.submit.pyFiles,/home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,/home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:01:15.298+0000] {spark_submit.py:491} INFO - Classpath elements:
[2025-04-13T20:01:15.298+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:01:15.298+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:01:15.299+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:01:15.299+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:01:16.438+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SparkContext: Running Spark version 3.5.5
[2025-04-13T20:01:16.439+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SparkContext: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:01:16.439+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SparkContext: Java version 11.0.26
[2025-04-13T20:01:16.470+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceUtils: ==============================================================
[2025-04-13T20:01:16.471+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-04-13T20:01:16.471+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceUtils: ==============================================================
[2025-04-13T20:01:16.472+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SparkContext: Submitted application: DailySearchVolumeAggregation
[2025-04-13T20:01:16.507+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-04-13T20:01:16.518+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceProfile: Limiting resource is cpu
[2025-04-13T20:01:16.518+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-04-13T20:01:16.581+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SecurityManager: Changing view acls to: airflow
[2025-04-13T20:01:16.581+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SecurityManager: Changing modify acls to: airflow
[2025-04-13T20:01:16.581+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SecurityManager: Changing view acls groups to:
[2025-04-13T20:01:16.582+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SecurityManager: Changing modify acls groups to:
[2025-04-13T20:01:16.582+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
[2025-04-13T20:01:16.971+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:16 INFO Utils: Successfully started service 'sparkDriver' on port 33211.
[2025-04-13T20:01:17.086+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkEnv: Registering MapOutputTracker
[2025-04-13T20:01:17.130+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkEnv: Registering BlockManagerMaster
[2025-04-13T20:01:17.144+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-04-13T20:01:17.145+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-04-13T20:01:17.151+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-04-13T20:01:17.170+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-16ceeeee-fb5b-4518-ae2a-594574123346
[2025-04-13T20:01:17.188+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-04-13T20:01:17.208+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-04-13T20:01:17.384+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-04-13T20:01:17.448+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-04-13T20:01:17.499+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at spark://0598501f5f50:33211/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744574476425
[2025-04-13T20:01:17.500+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at spark://0598501f5f50:33211/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744574476425
[2025-04-13T20:01:17.504+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744574476425
[2025-04-13T20:01:17.505+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Copying /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:01:17.543+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744574476425
[2025-04-13T20:01:17.544+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Copying /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:01:17.653+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Starting executor ID driver on host 0598501f5f50
[2025-04-13T20:01:17.653+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:01:17.654+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Java version 11.0.26
[2025-04-13T20:01:17.664+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-04-13T20:01:17.667+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@703a3be4 for default.
[2025-04-13T20:01:17.678+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744574476425
[2025-04-13T20:01:17.697+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar has been previously copied to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:01:17.724+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744574476425
[2025-04-13T20:01:17.728+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar has been previously copied to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:01:17.761+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Fetching spark://0598501f5f50:33211/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744574476425
[2025-04-13T20:01:17.812+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO TransportClientFactory: Successfully created connection to 0598501f5f50/172.18.0.9:33211 after 35 ms (0 ms spent in bootstraps)
[2025-04-13T20:01:17.832+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Fetching spark://0598501f5f50:33211/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/fetchFileTemp15327487634307358804.tmp
[2025-04-13T20:01:17.866+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/fetchFileTemp15327487634307358804.tmp has been previously copied to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:01:17.891+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Adding file:/tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/mysql_mysql-connector-java-8.0.28.jar to class loader default
[2025-04-13T20:01:17.892+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Fetching spark://0598501f5f50:33211/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744574476425
[2025-04-13T20:01:17.893+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Fetching spark://0598501f5f50:33211/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/fetchFileTemp17226568652737533988.tmp
[2025-04-13T20:01:17.906+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/fetchFileTemp17226568652737533988.tmp has been previously copied to /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:01:17.932+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Executor: Adding file:/tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/userFiles-ed4dde92-5934-4095-97a0-7ca46e04f563/com.google.protobuf_protobuf-java-3.11.4.jar to class loader default
[2025-04-13T20:01:17.946+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35251.
[2025-04-13T20:01:17.946+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO NettyBlockTransferService: Server created on 0598501f5f50:35251
[2025-04-13T20:01:17.950+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-04-13T20:01:17.955+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0598501f5f50, 35251, None)
[2025-04-13T20:01:17.959+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManagerMasterEndpoint: Registering block manager 0598501f5f50:35251 with 434.4 MiB RAM, BlockManagerId(driver, 0598501f5f50, 35251, None)
[2025-04-13T20:01:17.962+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0598501f5f50, 35251, None)
[2025-04-13T20:01:17.962+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0598501f5f50, 35251, None)
[2025-04-13T20:01:18.515+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:18 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-04-13T20:01:18.523+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:18 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2025-04-13T20:01:22.397+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO CodeGenerator: Code generated in 282.224001 ms
[2025-04-13T20:01:22.648+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Registering RDD 3 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-04-13T20:01:22.654+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Got map stage job 0 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:01:22.655+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:01:22.655+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Parents of final stage: List()
[2025-04-13T20:01:22.659+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:01:22.663+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:01:22.879+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.2 KiB, free 434.4 MiB)
[2025-04-13T20:01:22.920+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 434.4 MiB)
[2025-04-13T20:01:22.923+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0598501f5f50:35251 (size: 15.2 KiB, free: 434.4 MiB)
[2025-04-13T20:01:22.929+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:01:22.952+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:01:22.953+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-04-13T20:01:23.009+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0598501f5f50, executor driver, partition 0, PROCESS_LOCAL, 9329 bytes)
[2025-04-13T20:01:23.034+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-04-13T20:01:24.169+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:24 INFO CodeGenerator: Code generated in 117.579167 ms
[2025-04-13T20:01:24.270+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:24 INFO CodeGenerator: Code generated in 64.155334 ms
[2025-04-13T20:01:24.348+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:24 INFO CodeGenerator: Code generated in 55.02125 ms
[2025-04-13T20:01:24.405+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:24 INFO CodeGenerator: Code generated in 26.849334 ms
[2025-04-13T20:01:25.819+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:25 INFO JDBCRDD: closed connection
[2025-04-13T20:01:26.074+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO CodeGenerator: Code generated in 53.282083 ms
[2025-04-13T20:01:26.443+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2341 bytes result sent to driver
[2025-04-13T20:01:26.471+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3477 ms on 0598501f5f50 (executor driver) (1/1)
[2025-04-13T20:01:26.473+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-04-13T20:01:26.487+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO DAGScheduler: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 3.791 s
[2025-04-13T20:01:26.488+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO DAGScheduler: looking for newly runnable stages
[2025-04-13T20:01:26.491+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO DAGScheduler: running: Set()
[2025-04-13T20:01:26.491+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO DAGScheduler: waiting: Set()
[2025-04-13T20:01:26.492+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO DAGScheduler: failed: Set()
[2025-04-13T20:01:26.671+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-04-13T20:01:26.810+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO CodeGenerator: Code generated in 44.493708 ms
[2025-04-13T20:01:26.854+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:26 INFO CodeGenerator: Code generated in 30.472708 ms
[2025-04-13T20:01:27.052+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2025-04-13T20:01:27.058+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:01:27.059+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:01:27.059+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-04-13T20:01:27.062+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:01:27.066+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:01:27.123+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 49.8 KiB, free 434.3 MiB)
[2025-04-13T20:01:27.126+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 434.3 MiB)
[2025-04-13T20:01:27.127+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0598501f5f50:35251 (size: 22.6 KiB, free: 434.4 MiB)
[2025-04-13T20:01:27.127+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:01:27.131+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:01:27.132+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-04-13T20:01:27.146+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (0598501f5f50, executor driver, partition 0, NODE_LOCAL, 9494 bytes)
[2025-04-13T20:01:27.153+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2025-04-13T20:01:27.294+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO ShuffleBlockFetcherIterator: Getting 1 (504.1 KiB) non-empty blocks including 1 (504.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-04-13T20:01:27.297+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2025-04-13T20:01:27.357+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 48.986708 ms
[2025-04-13T20:01:27.513+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 32.776875 ms
[2025-04-13T20:01:27.547+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 14.207166 ms
[2025-04-13T20:01:27.567+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 12.546416 ms
[2025-04-13T20:01:27.585+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 15.193917 ms
[2025-04-13T20:01:27.618+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 28.044208 ms
[2025-04-13T20:01:27.814+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:27 INFO CodeGenerator: Code generated in 172.231417 ms
[2025-04-13T20:01:30.183+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4601 bytes result sent to driver
[2025-04-13T20:01:30.198+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 3055 ms on 0598501f5f50 (executor driver) (1/1)
[2025-04-13T20:01:30.198+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-04-13T20:01:30.202+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 3.108 s
[2025-04-13T20:01:30.209+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-04-13T20:01:30.210+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-04-13T20:01:30.213+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 3.160899 s
[2025-04-13T20:01:30.329+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO SparkContext: Invoking stop() from shutdown hook
[2025-04-13T20:01:30.329+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-04-13T20:01:30.345+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO SparkUI: Stopped Spark web UI at http://0598501f5f50:4040
[2025-04-13T20:01:30.358+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-04-13T20:01:30.402+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO MemoryStore: MemoryStore cleared
[2025-04-13T20:01:30.403+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO BlockManager: BlockManager stopped
[2025-04-13T20:01:30.410+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-04-13T20:01:30.412+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-04-13T20:01:30.451+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO SparkContext: Successfully stopped SparkContext
[2025-04-13T20:01:30.452+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO ShutdownHookManager: Shutdown hook called
[2025-04-13T20:01:30.452+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594/pyspark-297bd7fd-b3cd-4580-9df5-0d5f2df717f4
[2025-04-13T20:01:30.484+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-431e9e8a-323f-4f75-8351-681a8f6e72af
[2025-04-13T20:01:30.541+0000] {spark_submit.py:491} INFO - 25/04/13 20:01:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-b079cc77-c6ba-4a56-b157-53e93bf27594
[2025-04-13T20:01:30.686+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=daily_search_volume_aggregation, task_id=aggregate_search_volume, execution_date=20250413T190100, start_date=20250413T200101, end_date=20250413T200130
[2025-04-13T20:01:30.738+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-04-13T20:01:30.758+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-13T20:21:31.901+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:21:31.911+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:21:31.912+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-04-13T20:21:31.924+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): aggregate_search_volume> on 2025-04-13 19:01:00+00:00
[2025-04-13T20:21:31.927+0000] {standard_task_runner.py:57} INFO - Started process 189 to run task
[2025-04-13T20:21:31.931+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'daily_search_volume_aggregation', 'aggregate_search_volume', 'scheduled__2025-04-13T19:01:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/daily_search_volume_aggregation.py', '--cfg-path', '/tmp/tmp_5w3bfut']
[2025-04-13T20:21:31.933+0000] {standard_task_runner.py:85} INFO - Job 2: Subtask aggregate_search_volume
[2025-04-13T20:21:32.015+0000] {task_command.py:415} INFO - Running <TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [running]> on host e7bfbc91d8e7
[2025-04-13T20:21:32.155+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='daily_search_volume_aggregation' AIRFLOW_CTX_TASK_ID='aggregate_search_volume' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T19:01:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T19:01:00+00:00'
[2025-04-13T20:21:32.157+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2025-04-13T20:21:32.159+0000] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master spark:7077 --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.jars.packages=mysql:mysql-connector-java:8.0.28 --name arrow-spark --verbose /opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:21:35.726+0000] {spark_submit.py:491} INFO - Using properties file: null
[2025-04-13T20:21:35.896+0000] {spark_submit.py:491} INFO - Parsed arguments:
[2025-04-13T20:21:35.896+0000] {spark_submit.py:491} INFO - master                  spark:7077
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - remote                  null
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - deployMode              null
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - executorMemory          1g
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - executorCores           null
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - totalExecutorCores      null
[2025-04-13T20:21:35.897+0000] {spark_submit.py:491} INFO - propertiesFile          null
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - driverMemory            1g
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - driverCores             null
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - driverExtraClassPath    null
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - driverExtraLibraryPath  null
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - driverExtraJavaOptions  null
[2025-04-13T20:21:35.898+0000] {spark_submit.py:491} INFO - supervise               false
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - queue                   null
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - numExecutors            null
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - files                   null
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - pyFiles                 null
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - archives                null
[2025-04-13T20:21:35.899+0000] {spark_submit.py:491} INFO - mainClass               null
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - primaryResource         file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - name                    arrow-spark
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - childArgs               []
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - jars                    null
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - packages                mysql:mysql-connector-java:8.0.28
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - packagesExclusions      null
[2025-04-13T20:21:35.900+0000] {spark_submit.py:491} INFO - repositories            null
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - verbose                 true
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - Spark properties used, including those specified through
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - --conf and those from the properties file null:
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:21:35.901+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:21:35.902+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:21:35.902+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:21:35.902+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:21:36.052+0000] {spark_submit.py:491} INFO - :: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-04-13T20:21:36.313+0000] {spark_submit.py:491} INFO - Ivy Default Cache set to: /home/airflow/.ivy2/cache
[2025-04-13T20:21:36.314+0000] {spark_submit.py:491} INFO - The jars for the packages stored in: /home/airflow/.ivy2/jars
[2025-04-13T20:21:36.321+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java added as a dependency
[2025-04-13T20:21:36.324+0000] {spark_submit.py:491} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-df14def8-fe3e-4f8d-b750-0a4689b6daaa;1.0
[2025-04-13T20:21:36.324+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:21:38.072+0000] {spark_submit.py:491} INFO - found mysql#mysql-connector-java;8.0.28 in central
[2025-04-13T20:21:41.590+0000] {spark_submit.py:491} INFO - found com.google.protobuf#protobuf-java;3.11.4 in central
[2025-04-13T20:21:41.825+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar ...
[2025-04-13T20:21:43.840+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] mysql#mysql-connector-java;8.0.28!mysql-connector-java.jar (2223ms)
[2025-04-13T20:21:44.051+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.11.4/protobuf-java-3.11.4.jar ...
[2025-04-13T20:21:45.141+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] com.google.protobuf#protobuf-java;3.11.4!protobuf-java.jar(bundle) (1294ms)
[2025-04-13T20:21:45.148+0000] {spark_submit.py:491} INFO - :: resolution report :: resolve 5284ms :: artifacts dl 3540ms
[2025-04-13T20:21:45.149+0000] {spark_submit.py:491} INFO - :: modules in use:
[2025-04-13T20:21:45.150+0000] {spark_submit.py:491} INFO - com.google.protobuf#protobuf-java;3.11.4 from central in [default]
[2025-04-13T20:21:45.150+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java;8.0.28 from central in [default]
[2025-04-13T20:21:45.152+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:21:45.152+0000] {spark_submit.py:491} INFO - |                  |            modules            ||   artifacts   |
[2025-04-13T20:21:45.153+0000] {spark_submit.py:491} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-04-13T20:21:45.153+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:21:45.154+0000] {spark_submit.py:491} INFO - |      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
[2025-04-13T20:21:45.154+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:21:45.162+0000] {spark_submit.py:491} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-df14def8-fe3e-4f8d-b750-0a4689b6daaa
[2025-04-13T20:21:45.163+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:21:45.185+0000] {spark_submit.py:491} INFO - 2 artifacts copied, 0 already retrieved (4040kB/23ms)
[2025-04-13T20:21:45.483+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-04-13T20:21:45.857+0000] {spark_submit.py:491} INFO - Main class:
[2025-04-13T20:21:45.857+0000] {spark_submit.py:491} INFO - org.apache.spark.deploy.PythonRunner
[2025-04-13T20:21:45.857+0000] {spark_submit.py:491} INFO - Arguments:
[2025-04-13T20:21:45.858+0000] {spark_submit.py:491} INFO - file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:21:45.858+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:21:45.864+0000] {spark_submit.py:491} INFO - Spark config:
[2025-04-13T20:21:45.864+0000] {spark_submit.py:491} INFO - (spark.app.name,arrow-spark)
[2025-04-13T20:21:45.864+0000] {spark_submit.py:491} INFO - (spark.app.submitTime,1744575705814)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.files,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.master,spark:7077)
[2025-04-13T20:21:45.865+0000] {spark_submit.py:491} INFO - (spark.repl.local.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - (spark.submit.deployMode,client)
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - (spark.submit.pyFiles,/home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,/home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - Classpath elements:
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:21:45.866+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:21:47.068+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Running Spark version 3.5.5
[2025-04-13T20:21:47.069+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:21:47.069+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Java version 11.0.26
[2025-04-13T20:21:47.102+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceUtils: ==============================================================
[2025-04-13T20:21:47.103+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-04-13T20:21:47.104+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceUtils: ==============================================================
[2025-04-13T20:21:47.104+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Submitted application: DailySearchVolumeAggregation
[2025-04-13T20:21:47.138+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-04-13T20:21:47.149+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceProfile: Limiting resource is cpu
[2025-04-13T20:21:47.149+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-04-13T20:21:47.208+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SecurityManager: Changing view acls to: airflow
[2025-04-13T20:21:47.209+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SecurityManager: Changing modify acls to: airflow
[2025-04-13T20:21:47.209+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SecurityManager: Changing view acls groups to:
[2025-04-13T20:21:47.210+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SecurityManager: Changing modify acls groups to:
[2025-04-13T20:21:47.210+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
[2025-04-13T20:21:47.488+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO Utils: Successfully started service 'sparkDriver' on port 37455.
[2025-04-13T20:21:47.526+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkEnv: Registering MapOutputTracker
[2025-04-13T20:21:47.562+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkEnv: Registering BlockManagerMaster
[2025-04-13T20:21:47.576+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-04-13T20:21:47.576+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-04-13T20:21:47.581+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-04-13T20:21:47.598+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-08de2917-7b4b-492f-83f8-16ad56a524ee
[2025-04-13T20:21:47.619+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-04-13T20:21:47.649+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-04-13T20:21:47.815+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-04-13T20:21:47.900+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-04-13T20:21:47.976+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at spark://e7bfbc91d8e7:37455/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744575707053
[2025-04-13T20:21:47.977+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at spark://e7bfbc91d8e7:37455/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744575707053
[2025-04-13T20:21:47.983+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744575707053
[2025-04-13T20:21:47.985+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:47 INFO Utils: Copying /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:21:48.019+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744575707053
[2025-04-13T20:21:48.020+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: Copying /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:21:48.124+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Starting executor ID driver on host e7bfbc91d8e7
[2025-04-13T20:21:48.125+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:21:48.125+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Java version 11.0.26
[2025-04-13T20:21:48.140+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-04-13T20:21:48.143+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@41eb9cae for default.
[2025-04-13T20:21:48.156+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744575707053
[2025-04-13T20:21:48.197+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar has been previously copied to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:21:48.221+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744575707053
[2025-04-13T20:21:48.223+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar has been previously copied to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:21:48.250+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Fetching spark://e7bfbc91d8e7:37455/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744575707053
[2025-04-13T20:21:48.301+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO TransportClientFactory: Successfully created connection to e7bfbc91d8e7/172.18.0.9:37455 after 37 ms (0 ms spent in bootstraps)
[2025-04-13T20:21:48.317+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: Fetching spark://e7bfbc91d8e7:37455/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/fetchFileTemp739169157709006069.tmp
[2025-04-13T20:21:48.348+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/fetchFileTemp739169157709006069.tmp has been previously copied to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:21:48.372+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Adding file:/tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/com.google.protobuf_protobuf-java-3.11.4.jar to class loader default
[2025-04-13T20:21:48.372+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Fetching spark://e7bfbc91d8e7:37455/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744575707053
[2025-04-13T20:21:48.373+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: Fetching spark://e7bfbc91d8e7:37455/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/fetchFileTemp17687401444074593872.tmp
[2025-04-13T20:21:48.390+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/fetchFileTemp17687401444074593872.tmp has been previously copied to /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:21:48.414+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Executor: Adding file:/tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/userFiles-20a07434-2bcf-4c02-b57c-c4b0e09151b3/mysql_mysql-connector-java-8.0.28.jar to class loader default
[2025-04-13T20:21:48.429+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43785.
[2025-04-13T20:21:48.430+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO NettyBlockTransferService: Server created on e7bfbc91d8e7:43785
[2025-04-13T20:21:48.432+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-04-13T20:21:48.436+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e7bfbc91d8e7, 43785, None)
[2025-04-13T20:21:48.439+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO BlockManagerMasterEndpoint: Registering block manager e7bfbc91d8e7:43785 with 434.4 MiB RAM, BlockManagerId(driver, e7bfbc91d8e7, 43785, None)
[2025-04-13T20:21:48.443+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e7bfbc91d8e7, 43785, None)
[2025-04-13T20:21:48.443+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e7bfbc91d8e7, 43785, None)
[2025-04-13T20:21:48.980+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-04-13T20:21:48.987+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:48 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2025-04-13T20:21:53.122+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO CodeGenerator: Code generated in 280.348 ms
[2025-04-13T20:21:53.393+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Registering RDD 3 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-04-13T20:21:53.398+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Got map stage job 0 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:21:53.399+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:21:53.399+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Parents of final stage: List()
[2025-04-13T20:21:53.401+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:21:53.405+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:21:53.583+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.2 KiB, free 434.4 MiB)
[2025-04-13T20:21:53.619+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 434.4 MiB)
[2025-04-13T20:21:53.622+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e7bfbc91d8e7:43785 (size: 15.2 KiB, free: 434.4 MiB)
[2025-04-13T20:21:53.626+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:21:53.644+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:21:53.645+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-04-13T20:21:53.748+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e7bfbc91d8e7, executor driver, partition 0, PROCESS_LOCAL, 9329 bytes)
[2025-04-13T20:21:53.768+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-04-13T20:21:54.009+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 71.221 ms
[2025-04-13T20:21:54.071+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 44.50925 ms
[2025-04-13T20:21:54.124+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 39.731375 ms
[2025-04-13T20:21:54.166+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 21.578625 ms
[2025-04-13T20:21:54.168+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO JDBCRDD: closed connection
[2025-04-13T20:21:54.221+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 44.517792 ms
[2025-04-13T20:21:54.257+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2126 bytes result sent to driver
[2025-04-13T20:21:54.271+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 542 ms on e7bfbc91d8e7 (executor driver) (1/1)
[2025-04-13T20:21:54.273+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-04-13T20:21:54.281+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 0.860 s
[2025-04-13T20:21:54.283+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: looking for newly runnable stages
[2025-04-13T20:21:54.284+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: running: Set()
[2025-04-13T20:21:54.284+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: waiting: Set()
[2025-04-13T20:21:54.285+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: failed: Set()
[2025-04-13T20:21:54.326+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-04-13T20:21:54.406+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e7bfbc91d8e7:43785 in memory (size: 15.2 KiB, free: 434.4 MiB)
[2025-04-13T20:21:54.473+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 39.485208 ms
[2025-04-13T20:21:54.522+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 35.20125 ms
[2025-04-13T20:21:54.665+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2025-04-13T20:21:54.670+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:21:54.671+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:21:54.671+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-04-13T20:21:54.675+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:21:54.682+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:21:54.725+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 49.8 KiB, free 434.4 MiB)
[2025-04-13T20:21:54.728+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 434.3 MiB)
[2025-04-13T20:21:54.728+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e7bfbc91d8e7:43785 (size: 22.5 KiB, free: 434.4 MiB)
[2025-04-13T20:21:54.729+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:21:54.733+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:21:54.733+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-04-13T20:21:54.737+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (e7bfbc91d8e7, executor driver, partition 0, PROCESS_LOCAL, 9494 bytes)
[2025-04-13T20:21:54.737+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2025-04-13T20:21:54.869+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-04-13T20:21:54.871+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2025-04-13T20:21:54.910+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 36.054125 ms
[2025-04-13T20:21:54.967+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:54 INFO CodeGenerator: Code generated in 25.63 ms
[2025-04-13T20:21:55.005+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO CodeGenerator: Code generated in 16.254583 ms
[2025-04-13T20:21:55.029+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO CodeGenerator: Code generated in 18.238958 ms
[2025-04-13T20:21:55.048+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO CodeGenerator: Code generated in 16.971541 ms
[2025-04-13T20:21:55.070+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO CodeGenerator: Code generated in 19.809375 ms
[2025-04-13T20:21:55.158+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO CodeGenerator: Code generated in 65.956875 ms
[2025-04-13T20:21:55.191+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4558 bytes result sent to driver
[2025-04-13T20:21:55.299+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 561 ms on e7bfbc91d8e7 (executor driver) (1/1)
[2025-04-13T20:21:55.302+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-04-13T20:21:55.319+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 0.615 s
[2025-04-13T20:21:55.336+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-04-13T20:21:55.337+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-04-13T20:21:55.340+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 0.673857 s
[2025-04-13T20:21:55.488+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO SparkContext: Invoking stop() from shutdown hook
[2025-04-13T20:21:55.488+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-04-13T20:21:55.512+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO SparkUI: Stopped Spark web UI at http://e7bfbc91d8e7:4040
[2025-04-13T20:21:55.534+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-04-13T20:21:55.585+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO MemoryStore: MemoryStore cleared
[2025-04-13T20:21:55.586+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO BlockManager: BlockManager stopped
[2025-04-13T20:21:55.589+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-04-13T20:21:55.591+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-04-13T20:21:55.638+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO SparkContext: Successfully stopped SparkContext
[2025-04-13T20:21:55.639+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO ShutdownHookManager: Shutdown hook called
[2025-04-13T20:21:55.639+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-3dc5b4c6-a1fe-42aa-b017-7f73f248564d
[2025-04-13T20:21:55.671+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0
[2025-04-13T20:21:55.702+0000] {spark_submit.py:491} INFO - 25/04/13 20:21:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-131689f3-5131-4aa2-8e82-d9922dab16a0/pyspark-46c61387-3cd4-4707-a94c-d33c46a43aaa
[2025-04-13T20:21:55.841+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=daily_search_volume_aggregation, task_id=aggregate_search_volume, execution_date=20250413T190100, start_date=20250413T202131, end_date=20250413T202155
[2025-04-13T20:21:55.900+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-04-13T20:21:55.920+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-13T20:34:50.569+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:34:50.577+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [queued]>
[2025-04-13T20:34:50.578+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2025-04-13T20:34:50.593+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): aggregate_search_volume> on 2025-04-13 19:01:00+00:00
[2025-04-13T20:34:50.601+0000] {standard_task_runner.py:57} INFO - Started process 205 to run task
[2025-04-13T20:34:50.609+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'daily_search_volume_aggregation', 'aggregate_search_volume', 'scheduled__2025-04-13T19:01:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/daily_search_volume_aggregation.py', '--cfg-path', '/tmp/tmpzi3_f6dk']
[2025-04-13T20:34:50.611+0000] {standard_task_runner.py:85} INFO - Job 2: Subtask aggregate_search_volume
[2025-04-13T20:34:50.680+0000] {task_command.py:415} INFO - Running <TaskInstance: daily_search_volume_aggregation.aggregate_search_volume scheduled__2025-04-13T19:01:00+00:00 [running]> on host 32685a4881fe
[2025-04-13T20:34:50.774+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='daily_search_volume_aggregation' AIRFLOW_CTX_TASK_ID='aggregate_search_volume' AIRFLOW_CTX_EXECUTION_DATE='2025-04-13T19:01:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-13T19:01:00+00:00'
[2025-04-13T20:34:50.776+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2025-04-13T20:34:50.777+0000] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master spark:7077 --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.jars.packages=mysql:mysql-connector-java:8.0.28 --name arrow-spark --verbose /opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:34:54.127+0000] {spark_submit.py:491} INFO - Using properties file: null
[2025-04-13T20:34:54.275+0000] {spark_submit.py:491} INFO - Parsed arguments:
[2025-04-13T20:34:54.276+0000] {spark_submit.py:491} INFO - master                  spark:7077
[2025-04-13T20:34:54.276+0000] {spark_submit.py:491} INFO - remote                  null
[2025-04-13T20:34:54.276+0000] {spark_submit.py:491} INFO - deployMode              null
[2025-04-13T20:34:54.277+0000] {spark_submit.py:491} INFO - executorMemory          1g
[2025-04-13T20:34:54.277+0000] {spark_submit.py:491} INFO - executorCores           null
[2025-04-13T20:34:54.277+0000] {spark_submit.py:491} INFO - totalExecutorCores      null
[2025-04-13T20:34:54.277+0000] {spark_submit.py:491} INFO - propertiesFile          null
[2025-04-13T20:34:54.278+0000] {spark_submit.py:491} INFO - driverMemory            1g
[2025-04-13T20:34:54.278+0000] {spark_submit.py:491} INFO - driverCores             null
[2025-04-13T20:34:54.278+0000] {spark_submit.py:491} INFO - driverExtraClassPath    null
[2025-04-13T20:34:54.278+0000] {spark_submit.py:491} INFO - driverExtraLibraryPath  null
[2025-04-13T20:34:54.278+0000] {spark_submit.py:491} INFO - driverExtraJavaOptions  null
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - supervise               false
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - queue                   null
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - numExecutors            null
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - files                   null
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - pyFiles                 null
[2025-04-13T20:34:54.279+0000] {spark_submit.py:491} INFO - archives                null
[2025-04-13T20:34:54.280+0000] {spark_submit.py:491} INFO - mainClass               null
[2025-04-13T20:34:54.280+0000] {spark_submit.py:491} INFO - primaryResource         file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:34:54.280+0000] {spark_submit.py:491} INFO - name                    arrow-spark
[2025-04-13T20:34:54.280+0000] {spark_submit.py:491} INFO - childArgs               []
[2025-04-13T20:34:54.281+0000] {spark_submit.py:491} INFO - jars                    null
[2025-04-13T20:34:54.281+0000] {spark_submit.py:491} INFO - packages                mysql:mysql-connector-java:8.0.28
[2025-04-13T20:34:54.281+0000] {spark_submit.py:491} INFO - packagesExclusions      null
[2025-04-13T20:34:54.281+0000] {spark_submit.py:491} INFO - repositories            null
[2025-04-13T20:34:54.281+0000] {spark_submit.py:491} INFO - verbose                 true
[2025-04-13T20:34:54.282+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:34:54.282+0000] {spark_submit.py:491} INFO - Spark properties used, including those specified through
[2025-04-13T20:34:54.282+0000] {spark_submit.py:491} INFO - --conf and those from the properties file null:
[2025-04-13T20:34:54.282+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:34:54.282+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:34:54.283+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:34:54.283+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:34:54.283+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:34:54.444+0000] {spark_submit.py:491} INFO - :: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-04-13T20:34:54.686+0000] {spark_submit.py:491} INFO - Ivy Default Cache set to: /home/airflow/.ivy2/cache
[2025-04-13T20:34:54.687+0000] {spark_submit.py:491} INFO - The jars for the packages stored in: /home/airflow/.ivy2/jars
[2025-04-13T20:34:54.694+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java added as a dependency
[2025-04-13T20:34:54.696+0000] {spark_submit.py:491} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-dc6f18b7-2121-4f0a-a0e8-46867f3ec556;1.0
[2025-04-13T20:34:54.697+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:34:56.421+0000] {spark_submit.py:491} INFO - found mysql#mysql-connector-java;8.0.28 in central
[2025-04-13T20:34:59.898+0000] {spark_submit.py:491} INFO - found com.google.protobuf#protobuf-java;3.11.4 in central
[2025-04-13T20:35:00.116+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar ...
[2025-04-13T20:35:02.652+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] mysql#mysql-connector-java;8.0.28!mysql-connector-java.jar (2731ms)
[2025-04-13T20:35:02.854+0000] {spark_submit.py:491} INFO - downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.11.4/protobuf-java-3.11.4.jar ...
[2025-04-13T20:35:04.370+0000] {spark_submit.py:491} INFO - [SUCCESSFUL ] com.google.protobuf#protobuf-java;3.11.4!protobuf-java.jar(bundle) (1713ms)
[2025-04-13T20:35:04.378+0000] {spark_submit.py:491} INFO - :: resolution report :: resolve 5222ms :: artifacts dl 4460ms
[2025-04-13T20:35:04.379+0000] {spark_submit.py:491} INFO - :: modules in use:
[2025-04-13T20:35:04.380+0000] {spark_submit.py:491} INFO - com.google.protobuf#protobuf-java;3.11.4 from central in [default]
[2025-04-13T20:35:04.380+0000] {spark_submit.py:491} INFO - mysql#mysql-connector-java;8.0.28 from central in [default]
[2025-04-13T20:35:04.381+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:35:04.381+0000] {spark_submit.py:491} INFO - |                  |            modules            ||   artifacts   |
[2025-04-13T20:35:04.382+0000] {spark_submit.py:491} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-04-13T20:35:04.382+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:35:04.382+0000] {spark_submit.py:491} INFO - |      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
[2025-04-13T20:35:04.383+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-04-13T20:35:04.389+0000] {spark_submit.py:491} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-dc6f18b7-2121-4f0a-a0e8-46867f3ec556
[2025-04-13T20:35:04.391+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-04-13T20:35:04.419+0000] {spark_submit.py:491} INFO - 2 artifacts copied, 0 already retrieved (4040kB/30ms)
[2025-04-13T20:35:04.716+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-04-13T20:35:05.065+0000] {spark_submit.py:491} INFO - Main class:
[2025-04-13T20:35:05.066+0000] {spark_submit.py:491} INFO - org.apache.spark.deploy.PythonRunner
[2025-04-13T20:35:05.067+0000] {spark_submit.py:491} INFO - Arguments:
[2025-04-13T20:35:05.067+0000] {spark_submit.py:491} INFO - file:/opt/airflow/dags/spark/daily_aggregation.py
[2025-04-13T20:35:05.067+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:35:05.072+0000] {spark_submit.py:491} INFO - Spark config:
[2025-04-13T20:35:05.073+0000] {spark_submit.py:491} INFO - (spark.app.name,arrow-spark)
[2025-04-13T20:35:05.073+0000] {spark_submit.py:491} INFO - (spark.app.submitTime,1744576505025)
[2025-04-13T20:35:05.073+0000] {spark_submit.py:491} INFO - (spark.driver.memory,1g)
[2025-04-13T20:35:05.073+0000] {spark_submit.py:491} INFO - (spark.executor.memory,1g)
[2025-04-13T20:35:05.073+0000] {spark_submit.py:491} INFO - (spark.files,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:35:05.074+0000] {spark_submit.py:491} INFO - (spark.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:35:05.074+0000] {spark_submit.py:491} INFO - (spark.jars.packages,mysql:mysql-connector-java:8.0.28)
[2025-04-13T20:35:05.074+0000] {spark_submit.py:491} INFO - (spark.master,spark:7077)
[2025-04-13T20:35:05.074+0000] {spark_submit.py:491} INFO - (spark.repl.local.jars,file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:35:05.074+0000] {spark_submit.py:491} INFO - (spark.submit.deployMode,client)
[2025-04-13T20:35:05.075+0000] {spark_submit.py:491} INFO - (spark.submit.pyFiles,/home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar,/home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar)
[2025-04-13T20:35:05.075+0000] {spark_submit.py:491} INFO - Classpath elements:
[2025-04-13T20:35:05.075+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:35:05.075+0000] {spark_submit.py:491} INFO - file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:35:05.075+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:35:05.076+0000] {spark_submit.py:491} INFO - 
[2025-04-13T20:35:06.249+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkContext: Running Spark version 3.5.5
[2025-04-13T20:35:06.250+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkContext: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:35:06.250+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkContext: Java version 11.0.26
[2025-04-13T20:35:06.281+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceUtils: ==============================================================
[2025-04-13T20:35:06.281+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-04-13T20:35:06.282+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceUtils: ==============================================================
[2025-04-13T20:35:06.283+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkContext: Submitted application: DailySearchVolumeAggregation
[2025-04-13T20:35:06.318+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-04-13T20:35:06.328+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceProfile: Limiting resource is cpu
[2025-04-13T20:35:06.329+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-04-13T20:35:06.386+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SecurityManager: Changing view acls to: airflow
[2025-04-13T20:35:06.387+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SecurityManager: Changing modify acls to: airflow
[2025-04-13T20:35:06.387+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SecurityManager: Changing view acls groups to:
[2025-04-13T20:35:06.387+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SecurityManager: Changing modify acls groups to:
[2025-04-13T20:35:06.388+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
[2025-04-13T20:35:06.657+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO Utils: Successfully started service 'sparkDriver' on port 42721.
[2025-04-13T20:35:06.699+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkEnv: Registering MapOutputTracker
[2025-04-13T20:35:06.740+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkEnv: Registering BlockManagerMaster
[2025-04-13T20:35:06.753+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-04-13T20:35:06.754+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-04-13T20:35:06.759+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-04-13T20:35:06.776+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ac4d1c7-2c2e-42d8-80a5-55d96a9f2a65
[2025-04-13T20:35:06.791+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-04-13T20:35:06.808+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-04-13T20:35:06.956+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-04-13T20:35:07.015+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-04-13T20:35:07.066+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at spark://32685a4881fe:42721/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744576506237
[2025-04-13T20:35:07.067+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at spark://32685a4881fe:42721/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744576506237
[2025-04-13T20:35:07.071+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar at file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744576506237
[2025-04-13T20:35:07.073+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Copying /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:35:07.110+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar at file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744576506237
[2025-04-13T20:35:07.110+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Copying /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:35:07.217+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Starting executor ID driver on host 32685a4881fe
[2025-04-13T20:35:07.218+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: OS info Linux, 6.12.5-linuxkit, amd64
[2025-04-13T20:35:07.219+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Java version 11.0.26
[2025-04-13T20:35:07.233+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-04-13T20:35:07.238+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@140e24d3 for default.
[2025-04-13T20:35:07.251+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744576506237
[2025-04-13T20:35:07.273+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: /home/airflow/.ivy2/jars/mysql_mysql-connector-java-8.0.28.jar has been previously copied to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:35:07.299+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744576506237
[2025-04-13T20:35:07.302+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: /home/airflow/.ivy2/jars/com.google.protobuf_protobuf-java-3.11.4.jar has been previously copied to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:35:07.330+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Fetching spark://32685a4881fe:42721/jars/com.google.protobuf_protobuf-java-3.11.4.jar with timestamp 1744576506237
[2025-04-13T20:35:07.393+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO TransportClientFactory: Successfully created connection to 32685a4881fe/172.18.0.9:42721 after 49 ms (0 ms spent in bootstraps)
[2025-04-13T20:35:07.411+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Fetching spark://32685a4881fe:42721/jars/com.google.protobuf_protobuf-java-3.11.4.jar to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/fetchFileTemp2149730478829747540.tmp
[2025-04-13T20:35:07.437+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/fetchFileTemp2149730478829747540.tmp has been previously copied to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/com.google.protobuf_protobuf-java-3.11.4.jar
[2025-04-13T20:35:07.461+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Adding file:/tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/com.google.protobuf_protobuf-java-3.11.4.jar to class loader default
[2025-04-13T20:35:07.462+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Fetching spark://32685a4881fe:42721/jars/mysql_mysql-connector-java-8.0.28.jar with timestamp 1744576506237
[2025-04-13T20:35:07.463+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Fetching spark://32685a4881fe:42721/jars/mysql_mysql-connector-java-8.0.28.jar to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/fetchFileTemp10873687160203271202.tmp
[2025-04-13T20:35:07.484+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/fetchFileTemp10873687160203271202.tmp has been previously copied to /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/mysql_mysql-connector-java-8.0.28.jar
[2025-04-13T20:35:07.511+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Executor: Adding file:/tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/userFiles-a10cadec-c1e8-4d68-84ca-bca687f7a616/mysql_mysql-connector-java-8.0.28.jar to class loader default
[2025-04-13T20:35:07.524+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43215.
[2025-04-13T20:35:07.525+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO NettyBlockTransferService: Server created on 32685a4881fe:43215
[2025-04-13T20:35:07.527+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-04-13T20:35:07.532+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 32685a4881fe, 43215, None)
[2025-04-13T20:35:07.535+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO BlockManagerMasterEndpoint: Registering block manager 32685a4881fe:43215 with 434.4 MiB RAM, BlockManagerId(driver, 32685a4881fe, 43215, None)
[2025-04-13T20:35:07.538+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 32685a4881fe, 43215, None)
[2025-04-13T20:35:07.539+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 32685a4881fe, 43215, None)
[2025-04-13T20:35:08.130+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-04-13T20:35:08.138+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:08 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2025-04-13T20:35:12.192+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO CodeGenerator: Code generated in 257.761708 ms
[2025-04-13T20:35:12.461+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Registering RDD 3 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-04-13T20:35:12.467+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Got map stage job 0 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:35:12.468+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:35:12.468+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Parents of final stage: List()
[2025-04-13T20:35:12.470+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:35:12.475+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:35:12.642+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.2 KiB, free 434.4 MiB)
[2025-04-13T20:35:12.680+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 434.4 MiB)
[2025-04-13T20:35:12.682+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 32685a4881fe:43215 (size: 15.2 KiB, free: 434.4 MiB)
[2025-04-13T20:35:12.686+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:35:12.703+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:35:12.704+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-04-13T20:35:12.748+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (32685a4881fe, executor driver, partition 0, PROCESS_LOCAL, 9329 bytes)
[2025-04-13T20:35:12.768+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-04-13T20:35:13.209+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:13 INFO CodeGenerator: Code generated in 68.78 ms
[2025-04-13T20:35:13.269+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:13 INFO CodeGenerator: Code generated in 40.552 ms
[2025-04-13T20:35:13.334+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:13 INFO CodeGenerator: Code generated in 45.941584 ms
[2025-04-13T20:35:13.385+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:13 INFO CodeGenerator: Code generated in 22.283583 ms
[2025-04-13T20:35:14.490+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:14 INFO JDBCRDD: closed connection
[2025-04-13T20:35:14.717+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:14 INFO CodeGenerator: Code generated in 67.3905 ms
[2025-04-13T20:35:15.184+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2341 bytes result sent to driver
[2025-04-13T20:35:15.222+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2486 ms on 32685a4881fe (executor driver) (1/1)
[2025-04-13T20:35:15.224+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-04-13T20:35:15.238+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 2.748 s
[2025-04-13T20:35:15.239+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: looking for newly runnable stages
[2025-04-13T20:35:15.242+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: running: Set()
[2025-04-13T20:35:15.243+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: waiting: Set()
[2025-04-13T20:35:15.244+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: failed: Set()
[2025-04-13T20:35:15.332+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-04-13T20:35:15.487+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO CodeGenerator: Code generated in 52.486917 ms
[2025-04-13T20:35:15.531+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO CodeGenerator: Code generated in 33.52775 ms
[2025-04-13T20:35:15.744+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2025-04-13T20:35:15.750+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-04-13T20:35:15.750+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
[2025-04-13T20:35:15.751+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-04-13T20:35:15.753+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Missing parents: List()
[2025-04-13T20:35:15.758+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-04-13T20:35:15.832+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 49.8 KiB, free 434.3 MiB)
[2025-04-13T20:35:15.844+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 22.6 KiB, free 434.3 MiB)
[2025-04-13T20:35:15.847+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 32685a4881fe:43215 (size: 22.6 KiB, free: 434.4 MiB)
[2025-04-13T20:35:15.853+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-04-13T20:35:15.859+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-04-13T20:35:15.861+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-04-13T20:35:16.066+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 32685a4881fe:43215 in memory (size: 15.2 KiB, free: 434.4 MiB)
[2025-04-13T20:35:16.067+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (32685a4881fe, executor driver, partition 0, NODE_LOCAL, 9494 bytes)
[2025-04-13T20:35:16.076+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2025-04-13T20:35:16.243+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO ShuffleBlockFetcherIterator: Getting 1 (109.0 KiB) non-empty blocks including 1 (109.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-04-13T20:35:16.245+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
[2025-04-13T20:35:16.324+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 67.699834 ms
[2025-04-13T20:35:16.596+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 50.586333 ms
[2025-04-13T20:35:16.672+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 57.358208 ms
[2025-04-13T20:35:16.705+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 21.2215 ms
[2025-04-13T20:35:16.728+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 21.006209 ms
[2025-04-13T20:35:16.751+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 21.260417 ms
[2025-04-13T20:35:16.840+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:16 INFO CodeGenerator: Code generated in 69.109167 ms
[2025-04-13T20:35:17.587+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4601 bytes result sent to driver
[2025-04-13T20:35:17.606+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 1577 ms on 32685a4881fe (executor driver) (1/1)
[2025-04-13T20:35:17.609+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-04-13T20:35:17.613+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.822 s
[2025-04-13T20:35:17.625+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-04-13T20:35:17.626+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-04-13T20:35:17.627+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 1.882324 s
[2025-04-13T20:35:17.761+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO SparkContext: Invoking stop() from shutdown hook
[2025-04-13T20:35:17.762+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-04-13T20:35:17.775+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO SparkUI: Stopped Spark web UI at http://32685a4881fe:4040
[2025-04-13T20:35:17.790+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-04-13T20:35:17.831+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO MemoryStore: MemoryStore cleared
[2025-04-13T20:35:17.832+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO BlockManager: BlockManager stopped
[2025-04-13T20:35:17.835+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-04-13T20:35:17.838+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-04-13T20:35:17.927+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO SparkContext: Successfully stopped SparkContext
[2025-04-13T20:35:17.928+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO ShutdownHookManager: Shutdown hook called
[2025-04-13T20:35:17.929+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94/pyspark-1bf32653-5090-48af-b306-00e5a9b88790
[2025-04-13T20:35:17.974+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae6c8278-be07-4ab2-8fc0-6295489115a8
[2025-04-13T20:35:18.008+0000] {spark_submit.py:491} INFO - 25/04/13 20:35:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-572b456d-d304-4620-877f-d5a18c2bfd94
[2025-04-13T20:35:18.152+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=daily_search_volume_aggregation, task_id=aggregate_search_volume, execution_date=20250413T190100, start_date=20250413T203450, end_date=20250413T203518
[2025-04-13T20:35:18.188+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-04-13T20:35:18.204+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
